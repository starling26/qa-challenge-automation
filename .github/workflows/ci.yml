name: 🎭 QA Challenge - Automated Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Scope of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - smoke
          - critical
          - functional
          - validation
          - security

env:
  BASE_URL: https://parabank.parasoft.com/parabank

jobs:
  test-execution:
    name: 🧪 Test Execution
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        browser: [chromium, firefox, webkit]
        include:
          # Mobile testing
          - os: ubuntu-latest
            browser: mobile-chrome
          - os: ubuntu-latest
            browser: mobile-safari
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 🟢 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: 📦 Install dependencies
        run: npm ci

      - name: 🎭 Install Playwright browsers
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: 📁 Create test directories
        run: |
          mkdir -p test-results
          mkdir -p screenshots
          mkdir -p test-evidence

      - name: 🚀 Run All Tests
        if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == ''
        run: |
          echo "🔄 Running complete test suite"
          npx playwright test \
            --project=${{ matrix.browser }} \
            --reporter=html,junit \
            --output-dir=test-results
        continue-on-error: true

      - name: 💨 Run Smoke Tests
        if: github.event.inputs.test_scope == 'smoke'
        run: |
          echo "💨 Running smoke tests"
          npx playwright test \
            --project=${{ matrix.browser }} \
            --grep="TC01|TC02|TC04" \
            --reporter=html,junit \
            --output-dir=test-results
        continue-on-error: true

      - name: 🚨 Run Critical Tests
        if: github.event.inputs.test_scope == 'critical'
        run: |
          echo "🚨 Running critical tests"
          npx playwright test \
            --project=${{ matrix.browser }} \
            --grep="TC01|TC02|TC03|TC04|TC05|TC_AUTO_013|TC_AUTO_014" \
            --reporter=html,junit \
            --output-dir=test-results
        continue-on-error: true

      - name: ⚙️ Run Functional Tests
        if: github.event.inputs.test_scope == 'functional'
        run: |
          echo "⚙️ Running functional tests"
          npx playwright test \
            --project=${{ matrix.browser }} \
            --grep="TC01|TC04|TC09|TC10|TC11|TC12|TC_AUTO_013|TC_AUTO_014|TC_AUTO_017|TC_AUTO_018" \
            --reporter=html,junit \
            --output-dir=test-results
        continue-on-error: true

      - name: ✅ Run Validation Tests
        if: github.event.inputs.test_scope == 'validation'
        run: |
          echo "✅ Running validation tests"
          npx playwright test \
            --project=${{ matrix.browser }} \
            --grep="TC02|TC03|TC07|TC13A" \
            --reporter=html,junit \
            --output-dir=test-results
        continue-on-error: true

      - name: 🔒 Run Security Tests
        if: github.event.inputs.test_scope == 'security'
        run: |
          echo "🔒 Running security tests"
          npx playwright test \
            --project=${{ matrix.browser }} \
            --grep="TC05|TC08" \
            --reporter=html,junit \
            --output-dir=test-results
        continue-on-error: true

      - name: 📊 Generate Test Report
        run: |
          echo "# 🧪 Test Execution Report" > test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "**Platform**: ${{ matrix.os }}" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "**Browser**: ${{ matrix.browser }}" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "**Scope**: ${{ github.event.inputs.test_scope || 'all' }}" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "**Date**: $(date)" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "**Commit**: ${{ github.sha }}" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          
          echo "## 🎯 Test Results" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          
          if [ -d "test-results" ]; then
            # Count passed and failed tests from JUnit XML
            if find test-results -name "*.xml" -type f | head -1 | xargs test -f; then
              passed_tests=$(find test-results -name "*.xml" -exec grep -l 'failures="0"' {} \; | wc -l)
              failed_tests=$(find test-results -name "*.xml" -exec grep -l 'failures="[1-9]' {} \; | wc -l)
              total_tests=$((passed_tests + failed_tests))
              
              echo "- ✅ **Passed**: $passed_tests" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
              echo "- ❌ **Failed**: $failed_tests" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
              echo "- 📊 **Total**: $total_tests" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
              
              if [ "$total_tests" -gt 0 ]; then
                pass_rate=$(echo "scale=1; ($passed_tests * 100) / $total_tests" | bc)
                echo "- 📈 **Pass Rate**: ${pass_rate}%" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
              fi
            else
              echo "- ⚠️ **Status**: No JUnit results found" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
            fi
          else
            echo "- ⚠️ **Status**: No test results directory found" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          fi
          
          echo "" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "## 🛠️ Environment Details" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "- **OS**: ${{ matrix.os }}" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "- **Browser**: ${{ matrix.browser }}" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "- **Node**: $(node --version)" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md
          echo "- **Playwright**: $(npx playwright --version)" >> test-report-${{ matrix.os }}-${{ matrix.browser }}.md

      - name: 📸 Capture test evidence
        if: failure()
        run: |
          echo "📸 Collecting test evidence..."
          
          if [ -d "test-results" ]; then
            find test-results -name "*.png" -exec cp {} test-evidence/ \; 2>/dev/null || true
            find test-results -name "*.webm" -exec cp {} test-evidence/ \; 2>/dev/null || true
            echo "Evidence collected in test-evidence/"
            ls -la test-evidence/ || echo "No evidence files found"
          fi

      - name: 📊 Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.os }}-${{ matrix.browser }}
          path: |
            test-results/
            test-evidence/
            screenshots/
            test-report-${{ matrix.os }}-${{ matrix.browser }}.md
            playwright-report/
          retention-days: 30

  test-summary:
    name: 📋 Test Summary
    runs-on: ubuntu-latest
    needs: test-execution
    if: always()
    outputs:
      success_rate: ${{ steps.calculate_summary.outputs.success_rate }}
      overall_status: ${{ steps.calculate_summary.outputs.overall_status }}
    
    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v4

      - name: 📥 Download all test results
        uses: actions/download-artifact@v4
        with:
          path: ./all-test-results

      - name: 📊 Calculate test summary
        id: calculate_summary
        run: |
          echo "# 📋 QA Challenge - Test Summary Report" > master-test-summary.md
          echo "" >> master-test-summary.md
          echo "**Generated**: $(date)" >> master-test-summary.md
          echo "**Workflow**: [${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> master-test-summary.md
          echo "**Test Scope**: ${{ github.event.inputs.test_scope || 'all' }}" >> master-test-summary.md
          echo "**Trigger**: ${{ github.event_name }}" >> master-test-summary.md
          echo "" >> master-test-summary.md
          
          echo "## 🎯 Execution Matrix" >> master-test-summary.md
          echo "" >> master-test-summary.md
          echo "| Platform | Browser | Status | Details |" >> master-test-summary.md
          echo "|----------|---------|--------|---------|" >> master-test-summary.md
          
          total_combinations=0
          successful_combinations=0
          
          for result_dir in ./all-test-results/test-results-*; do
            if [ -d "$result_dir" ]; then
              total_combinations=$((total_combinations + 1))
              
              basename_dir=$(basename "$result_dir")
              platform=$(echo "$basename_dir" | sed 's/test-results-\([^-]*\)-\(.*\)/\1/')
              browser=$(echo "$basename_dir" | sed 's/test-results-\([^-]*\)-\(.*\)/\2/')
              
              # Check for evidence of failure
              if [ -d "$result_dir/test-evidence" ] && [ "$(ls -A "$result_dir/test-evidence" 2>/dev/null)" ]; then
                status="❌ Failed"
                details="[Evidence](./$basename_dir/test-evidence/)"
              else
                status="✅ Passed"
                successful_combinations=$((successful_combinations + 1))
                details="[Report](./$basename_dir/)"
              fi
              
              echo "| $platform | $browser | $status | $details |" >> master-test-summary.md
            fi
          done
          
          # Calculate success rate
          success_rate=0
          if [ "$total_combinations" -gt 0 ]; then
            success_rate=$(echo "scale=1; ($successful_combinations * 100) / $total_combinations" | bc -l)
          fi
          
          echo "" >> master-test-summary.md
          echo "## 📊 Summary Statistics" >> master-test-summary.md
          echo "- **Total Test Combinations**: $total_combinations" >> master-test-summary.md
          echo "- **Successful Runs**: $successful_combinations" >> master-test-summary.md
          echo "- **Failed Runs**: $((total_combinations - successful_combinations))" >> master-test-summary.md
          echo "- **Success Rate**: ${success_rate}%" >> master-test-summary.md
          echo "" >> master-test-summary.md
          
          # Determine overall status
          if [ "$(echo "$success_rate == 100" | bc -l)" = "1" ]; then
            overall_status="🎉 ALL TESTS PASSED"
          elif [ "$(echo "$success_rate >= 80" | bc -l)" = "1" ]; then
            overall_status="⚠️ MOSTLY PASSED"
          else
            overall_status="❌ MULTIPLE FAILURES"
          fi
          
          echo "## 🏆 Overall Result: $overall_status" >> master-test-summary.md
          echo "" >> master-test-summary.md
          
          # Test categories
          echo "## 📚 Test Categories Covered" >> master-test-summary.md
          echo "- **Functional Tests**: Form interactions, navigation, user workflows" >> master-test-summary.md
          echo "- **Validation Tests**: Input validation, error handling, boundary conditions" >> master-test-summary.md
          echo "- **Security Tests**: XSS prevention, input sanitization" >> master-test-summary.md
          echo "- **UI Tests**: Element visibility, accessibility" >> master-test-summary.md
          echo "- **Cross-browser Tests**: Chromium, Firefox, WebKit, Mobile browsers" >> master-test-summary.md
          echo "" >> master-test-summary.md
          
          echo "success_rate=$success_rate" >> $GITHUB_OUTPUT
          echo "overall_status=$overall_status" >> $GITHUB_OUTPUT

      - name: 📊 Upload master summary
        uses: actions/upload-artifact@v4
        with:
          name: master-test-summary
          path: master-test-summary.md
          retention-days: 90

      - name: 💬 Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summaryContent = fs.readFileSync('master-test-summary.md', 'utf8');
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## 🧪 Automated Test Results\n\n${summaryContent}`
            });

  check-quality-gate:
    name: 🚦 Quality Gate
    runs-on: ubuntu-latest
    needs: test-summary
    if: always()
    
    steps:
      - name: ❌ Fail if quality gate not met
        if: contains(needs.test-summary.outputs.overall_status, 'FAILURES')
        run: |
          echo "❌ Quality gate failed!"
          echo "Overall status: ${{ needs.test-summary.outputs.overall_status }}"
          echo "Success rate: ${{ needs.test-summary.outputs.success_rate }}%"
          echo "🔍 Check the test artifacts for detailed failure analysis"
          exit 1
          
      - name: ✅ Quality gate passed
        if: contains(needs.test-summary.outputs.overall_status, 'PASSED')
        run: |
          echo "✅ Quality gate passed!"
          echo "Overall status: ${{ needs.test-summary.outputs.overall_status }}"
          echo "Success rate: ${{ needs.test-summary.outputs.success_rate }}%"
          echo "🎉 All tests are running successfully across all platforms!"
